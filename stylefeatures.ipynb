{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport shutil\nimport glob\nimport torch\nfrom torchvision.models import vgg19\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torchvision.models.feature_extraction import create_feature_extractor\nimport torch.nn.functional as F\n\ndef image_loader(image_name):\n    image = Image.open(image_name)\n    # fake batch dimension required to fit network's input dimensions\n    image = loader(image).unsqueeze(0)\n    return image.to(device)\n\nnormalization_mean = [0.485, 0.456, 0.406]\nnormalization_std = [0.229, 0.224, 0.225]\n\nresolution = (256,256)\n\nloader  = transforms.Compose([transforms.ToTensor(), \n                                 transforms.Normalize(mean = normalization_mean, std = normalization_std),\n                                 transforms.Resize(resolution[0]),\n                                 transforms.CenterCrop(resolution)])\n\ndevice='cpu'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-10T10:22:25.210812Z","iopub.execute_input":"2022-12-10T10:22:25.211451Z","iopub.status.idle":"2022-12-10T10:22:25.222897Z","shell.execute_reply.started":"2022-12-10T10:22:25.211414Z","shell.execute_reply":"2022-12-10T10:22:25.221305Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def store_styles(folder):\n    \"\"\"\n    Gets a list of styles and filenames from the folder (location)\n    Param:\n        folder: a location with format:\n            >Bad\n            >>images\n            >Good\n            >>class_1\n            >>>images\n            >>class_2\n            >>>images\n            ...\n            where images = many files\n    Returns:\n        style_index: a list of styles\n        result: a data dictionary where each filename is key, and the values is a list of 0/1 where 1 is the label for which the style is good\n    \"\"\"\n    \n    conflict_exception = False\n    \n    result = dict()\n\n    good_folder = os.path.join(folder, \"Good\")\n    #get all styles in the good folder\n    styles = sorted(glob.glob(os.path.join(good_folder, \"*\")))\n    \n    #create list of styles which will help us associate a 1 at an index with a class\n    #e.g. [1,1,1] is associated with good for \"ACT-410\", \"AWA\", \"INS\"\n    #[0,0,1] is associated with good for \"INS\"\n    #[0,0,0] is associated with bad style\n    style_index = list()\n    for style in styles:\n        style_name = os.path.basename(style)\n        style_index.append(style_name)\n    \n    #add good styles to our data\n    for style in styles:\n        style_name = os.path.basename(style)\n        #get images in all styles:\n        style_images = sorted(glob.glob(os.path.join(style, \"*\")))\n        for style_image in style_images:\n            \n            #extract filename from path\n            style_filename = os.path.basename(style_image)\n            #if style filename is not already added to dictionary, create new dictionary entry\n            if style_filename not in result:\n                #create label list\n                result[style_filename] = [0] * len(style_index)\n            #set label of style index to 1\n            result[style_filename][style_index.index(style_name)] += 1\n    \n    #add bad styles to our data\n    bad_folder = os.path.join(folder, \"Bad\")\n    #get all files in the folder of bad styles\n    bad_images = sorted(glob.glob(os.path.join(bad_folder, \"*\")))\n    \n    for bad_image in bad_images:\n        bad_style_filename = os.path.basename(bad_image)\n        if bad_style_filename in result:\n            print(\"Conflict found with image %s!\" %bad_style_filename)\n            conflict_exception=True\n            \n        result[bad_style_filename] = [0] * len(style_index)\n\n    if conflict_exception:\n        raise Exception(\"You can't have a bad style image which is also good!\")\n        return \"BIG_ERROR\"\n        \n    return style_index, result","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:22:25.585895Z","iopub.execute_input":"2022-12-10T10:22:25.586771Z","iopub.status.idle":"2022-12-10T10:22:25.600386Z","shell.execute_reply.started":"2022-12-10T10:22:25.586720Z","shell.execute_reply":"2022-12-10T10:22:25.599313Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"style_folder = \"/kaggle/input/goodbadstyles/KhuongsGoodAndBadStyles/GoodAndBadStyles\"\n\nstyle_index, result = store_styles(style_folder)","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:22:54.154442Z","iopub.execute_input":"2022-12-10T10:22:54.154942Z","iopub.status.idle":"2022-12-10T10:22:54.168582Z","shell.execute_reply.started":"2022-12-10T10:22:54.154908Z","shell.execute_reply":"2022-12-10T10:22:54.166693Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"for data in result:\n    print(data)\n    print(result[data])\n    break","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:22:54.650634Z","iopub.execute_input":"2022-12-10T10:22:54.651048Z","iopub.status.idle":"2022-12-10T10:22:54.658904Z","shell.execute_reply.started":"2022-12-10T10:22:54.651016Z","shell.execute_reply":"2022-12-10T10:22:54.657332Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"0000218244_OG.jpeg\n[1, 0, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(result)\nprint(style_index)\nprint(result[list(result.keys())[0]])\n\n#how many styles in each class\nclass_count = dict({\"000\" : 0, \"001\" : 0, \"010\" : 0, \"011\" : 0, \"100\" : 0, \"101\": 0, \"110\" : 0, \"111\" : 0})\n\nfor data in result.values():\n    class_ = str(data[0]) + str(data[1]) + str(data[2])\n    class_count[class_] = class_count[class_] + 1\nprint(class_count)","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:22:55.019350Z","iopub.execute_input":"2022-12-10T10:22:55.020161Z","iopub.status.idle":"2022-12-10T10:22:55.030377Z","shell.execute_reply.started":"2022-12-10T10:22:55.020125Z","shell.execute_reply":"2022-12-10T10:22:55.029364Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"{'0000218244_OG.jpeg': [1, 0, 0], '0719_Hurricane_Florence_1600.jpg': [1, 1, 1], '1024px-immortels_-_dynamosquito.jpg': [1, 0, 1], '121089-050-42fc73b7.jpg': [1, 0, 0], '14ac64752b04d2b7ba03023ad1309891.jpeg': [1, 0, 1], '1666527382925_tdy_sun_hurricane_roslyn_mexico_221023_1920x1080-87ukc2.jpg': [1, 1, 1], '220908145343-weather-satellite-hurricane-earl-20220908.jpg': [1, 1, 1], '3879022x.jpeg': [1, 0, 1], '6110392390_4ccdb68ec4_b.jpeg': [1, 0, 1], '8345-Babylon-lion.jpg': [1, 0, 1], '93f0869765e804ce24fb7a6638672dae--brick.jpg': [1, 1, 0], 'A-NASA-Joaquin.jpeg': [1, 1, 1], 'BwSgJO3IIAABWmE.jpeg': [1, 1, 1], 'Claude_Monet_Water_Lilies_1917-1919.jpeg': [1, 0, 1], 'EsgkveaXUAI1-sc.jpg': [1, 1, 1], 'IMG_3458.jpeg': [1, 0, 1], 'Michael_2018-10-10_1715Z_cropped.jpg': [1, 1, 1], 'Monet-04062015095820.jpeg': [1, 0, 0], 'autumn_rhythm-pollock1.jpg': [1, 0, 0], 'babylonian-dragon-01-left-weston-westmoreland.jpg': [1, 1, 0], 'colorful-autumn-leaves-background.jpg': [1, 0, 0], 'colorful-fabric-leaf-orange-red-by-Elizabeths-Studio-Landscape-Medley-208435-2.jpeg': [1, 1, 0], 'dsc09716-editar.jpg': [1, 1, 1], 'fa22d6dfbf0abb19a6c72f8fdbea2f1e.jpg': [1, 1, 0], 'fitow_tmo_2007247_lrg.jpg': [1, 0, 1], 'ian.7.jpg': [1, 0, 1], 'il_fullxfull525984995_d866.jpg': [1, 0, 1], 'image-asset.jpeg': [1, 0, 1], 'katewilson.jpg': [1, 0, 1], 'maxresdefault.jpg': [1, 1, 0], 'nortwhjgh4a91.jpeg': [1, 0, 0], 'packed-Autumn-leaf-fabric-by-Timeless-Treasures-232818-1.jpg': [1, 1, 1], 's-l1600.jpeg': [1, 0, 1], 'seawifs_olga_lrg.jpg': [1, 1, 1], 'skynews-florida-hurricane-nicole_5960570.jpg': [1, 0, 1], 'unnamed.jpeg': [1, 0, 1], 'water-lilies-red-claude-monet-1919-06ca30b4.jpeg': [1, 0, 1], '6881071-ZHWMOZLA-7.jpg': [0, 1, 0], 'Jackson_Pollock.jpg': [0, 1, 1], 'screen-6.jpeg': [0, 1, 0], 'secret-garden-flowers-natalie-holland.jpg': [0, 1, 0], 'madpoint-fun-floral-by-yaroslav-slavinskiy-wallpaper.png': [0, 0, 1], '0.jpg': [0, 0, 0], '1.jpg': [0, 0, 0], '10742886.jpg': [0, 0, 0], '10cd65a85382e93db319fcdf486a91b9--charcoal-sketch-lobe.jpg': [0, 0, 0], '122024114-red-flame-fire-texture-background.jpeg': [0, 0, 0], '13215095_dripping-5.jpg': [0, 0, 0], '14364437_style-of-jackson-pollock.jpg': [0, 0, 0], '2.jpeg': [0, 0, 0], '2797df19a8954d871fe31efbd8c8c4be.jpg': [0, 0, 0], '3.jpg': [0, 0, 0], '3b24418249188d57d76bab506549ceec.jpg': [0, 0, 0], '5cd7222fc34886942aef5c59f6c99825.jpg': [0, 0, 0], '6692775.jpg': [0, 0, 0], '684045.jpg': [0, 0, 0], '6c983fab1dd851e525638251218626dc.jpg': [0, 0, 0], '7.jpg': [0, 0, 0], '7932238_dripping.jpg': [0, 0, 0], '8880934_scream.jpg': [0, 0, 0], '93537.jpg': [0, 0, 0], 'DyxiOhdU0AAP-Wu.jpg': [0, 0, 0], 'EJ8sWvJWoAAP-L0.jpg': [0, 0, 0], 'Esn6jljXcAAltGE.jpg': [0, 0, 0], 'Flaming-Gold.jpg': [0, 0, 0], 'IMG_1874.JPG': [0, 0, 0], 'IMG_4022.jpg': [0, 0, 0], 'a106acf6786aee0b13e2ec4b5689e59a.jpg': [0, 0, 0], 'abstract-matrix-background-green-black-colors_108964-3.webp': [0, 0, 0], 'blaze-fire-flame.jpg': [0, 0, 0], 'code_matrix_numbers_200408_1920x1200.jpg': [0, 0, 0], 'depositphotos_15642823-stock-photo-seamless-fire-texture.jpg': [0, 0, 0], 'fall-maple-leaves-rustic-wooden-table_112977-2239.jpeg': [0, 0, 0], 'famous-jackson-pollock-paintings-1200x800.jpg': [0, 0, 0], 'fine-neon-green-matrix-xjxdv19fpcxfbord-xjxdv19fpcxfbord.jpg': [0, 0, 0], 'fire_texture1431.jpg': [0, 0, 0], 'gandhi-drip-painting.jpg': [0, 0, 0], 'il_fullxful394566304_pe6u.jpg': [0, 0, 0], 'images (1).jpeg': [0, 0, 0], 'images.jpeg': [0, 0, 0], 'jakson-pullock-16505422953x2.jpeg': [0, 0, 0], 'maple-leaves.jpg': [0, 0, 0], 'matrix-binary-code-green-d3.jpg': [0, 0, 0], 'matrix-code-numbers-green.jpg': [0, 0, 0], 'neon-green-honeycomb-matrix-6innruoaax2z3uxw-6innruoaax2z3uxw.jpg': [0, 0, 0], 'off-white-Timeless-Treasures-Autumn-maple-leaf-fabric-232819-2.jpg': [0, 0, 0], 'thumb-1920-77840.jpg': [0, 0, 0], 'vector-fire-flame-texture-background_88211-362.jpeg': [0, 0, 0], 'wp9929423.png': [0, 0, 0], 'wp9929428.jpg': [0, 0, 0]}\n['ACT-410', 'AWA', 'INS']\n[1, 0, 0]\n{'000': 48, '001': 1, '010': 3, '011': 1, '100': 6, '101': 16, '110': 5, '111': 10}\n","output_type":"stream"}]},{"cell_type":"code","source":"pretrained_vgg = vgg19(pretrained=True)\ntorch.set_grad_enabled(False)\nlayers = [(name,module) for name, module in pretrained_vgg.named_modules()]\nreturn_nodes = {\n            #experiment 1-2\n            layers[6][0]: \"feature\"\n            #experiment 3 #conv1-1\n            #layers[3][0]: \"feature\"\n            #experiment 4 #conv2-1\n            #layers[8][0]: \"feature\"\n            #experiment 5 #pooling 2\n            #layers[11][0]: \"feature\"\n            #experiment 9 #pooling 3\n            #layers[20][0]: \"feature\"\n        }\nintermediate_vgg = create_feature_extractor(pretrained_vgg, return_nodes=return_nodes)\n    \ndef process_1(file_location):   \n    img = image_loader(file_location)\n    features = intermediate_vgg(img)\n    return features['feature']\n\ndef process_2(file_location):\n    \"\"\"placeholder function\"\"\"\n    return np.random.rand(1)\n\n'''\ndef process_1(file_location):\n    \"\"\"features about texture using fourier transform (FFT)\"\"\"\n    img = np.array(Image.open(file_location).convert('L'))    \n    return np.log(np.abs(fft.fftshift(fft.fft2(img, s=(20,20)))) + 10e-10).flatten()\n\ndef process_2(file_location):\n    \"\"\"features about histogram\"\"\"    \n    img_rgb = np.array(Image.open(file_location).convert('RGB'))\n    img_hsv = np.array(Image.open(file_location).convert('HSV'))      \n    return np.concatenate([np.mean(img_rgb, axis=(0, 1)), np.std(img_rgb, axis=(0, 1)), np.mean(img_hsv, axis=(0, 1)), np.std(img_hsv, axis=(0, 1))])\n'''\n\ndef process_image(file_location):\n    feature_0 = process_1(file_location)\n    #feature_1 = process_2(file_location)\n    return np.array(feature_0).flatten()\n\ndef process_dataset(style_index, file_dict, filenames, data_path):\n    \"\"\"\n    Generates the train data for the given styles.\n    Parameters:\n        style_index: a list of styles to help identify which index of the list associated with a file belongs to a specific \"working style\";\n                will be used to find the path of styles\n        file_dict: a dictionary where the keys are all filenames and the values are a list which state the labels of the the filename\n        filenames: a(n ordered) list of filenames, to ensure that the output will have the train data in the expected order\n        data_path: path to the dataset in which folders [\"Bad\", \"Good\"] are found\n    returns:\n        X: a numpy array which is the train data for all the files in file_dict\n    \"\"\"\n    X = []\n    for filename in filenames:\n        #get list of classes for current filename\n        current_labels = file_dict[filename]\n        try:\n            #get the location for the first class for which the style is good\n            #due to structure of data, good styles are found in \"\n            label_index = current_labels.index(1)\n            \n            style_class = style_index[label_index]\n            \n            current_class_location = os.path.join(data_path, \"Good\", style_class)\n        except ValueError:\n            #if not found, then the class is \"bad style\"\n            current_class_location = os.path.join(data_path, \"Bad\")\n        \n        current_file_location = os.path.join(current_class_location, filename)\n        X.append(process_image(current_file_location))\n        \n    return np.array(X)\n        \n        \n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:22:56.108560Z","iopub.execute_input":"2022-12-10T10:22:56.109251Z","iopub.status.idle":"2022-12-10T10:23:32.845939Z","shell.execute_reply.started":"2022-12-10T10:22:56.109210Z","shell.execute_reply":"2022-12-10T10:23:32.844654Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad6a91df62a6402090b9e5643d7d0b3b"}},"metadata":{}}]},{"cell_type":"code","source":"print(layers[20])\n#print(layers)","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:23:32.848640Z","iopub.execute_input":"2022-12-10T10:23:32.849209Z","iopub.status.idle":"2022-12-10T10:23:32.856756Z","shell.execute_reply.started":"2022-12-10T10:23:32.849169Z","shell.execute_reply":"2022-12-10T10:23:32.855310Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"('features.18', MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))\n","output_type":"stream"}]},{"cell_type":"code","source":"negatives = 0\npositives = 0\nfor key in result.keys():\n    if sum(result[key])>0:\n        positives+=1\n    else:\n        negatives+=1\n        \nprint(positives)\nprint(negatives)","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:23:32.858859Z","iopub.execute_input":"2022-12-10T10:23:32.859232Z","iopub.status.idle":"2022-12-10T10:23:33.620993Z","shell.execute_reply.started":"2022-12-10T10:23:32.859200Z","shell.execute_reply":"2022-12-10T10:23:33.619226Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"42\n48\n","output_type":"stream"}]},{"cell_type":"code","source":"#get filenames into a sorted list\nfilenames = sorted(list(result.keys()))\n#gets data from the filenames\nX = process_dataset(style_index, result, filenames, style_folder)\n\nY = []\n#get the true labels\nfor filename in filenames:\n    #reminder: result is a dictionary where each filename has a label [int,int,int] associated with it\n    #now we do binary classification so we only use one class\n    if sum(result[filename])>0:\n        Y.append(1)\n    else:\n        Y.append(0)\nY = np.array(Y)\n\nfrom sklearn.model_selection import train_test_split\n#do the train/test split\ntrain_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:23:33.624491Z","iopub.execute_input":"2022-12-10T10:23:33.625526Z","iopub.status.idle":"2022-12-10T10:23:56.800886Z","shell.execute_reply.started":"2022-12-10T10:23:33.625400Z","shell.execute_reply":"2022-12-10T10:23:56.799065Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import balanced_accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:23:56.802687Z","iopub.execute_input":"2022-12-10T10:23:56.804151Z","iopub.status.idle":"2022-12-10T10:23:56.893273Z","shell.execute_reply.started":"2022-12-10T10:23:56.804086Z","shell.execute_reply":"2022-12-10T10:23:56.891666Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"clf = SVC()\nclf.fit(train_X, train_Y)\npreds = clf.predict(test_X)\nmetric = balanced_accuracy_score(test_Y, preds)\nprint(\"Balanced accuracy score = %f\" %metric)","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:23:56.895636Z","iopub.execute_input":"2022-12-10T10:23:56.896420Z","iopub.status.idle":"2022-12-10T10:24:06.359656Z","shell.execute_reply.started":"2022-12-10T10:23:56.896383Z","shell.execute_reply":"2022-12-10T10:24:06.357988Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Balanced accuracy score = 0.887500\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict_styles(clf, folder):\n    \"\"\"\n    Given a folder containing images, predict whether each style image is good or not.\n    \"\"\"\n    #get a list of images\n    predictions = []\n    images = sorted(glob.glob(os.path.join(folder, \"*\")))\n    for image in images:\n        print(image)\n        X = process_image(image).reshape(1, -1)\n        prediction = clf.predict(X)\n        predictions.append([image, prediction])\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:24:06.362238Z","iopub.execute_input":"2022-12-10T10:24:06.364144Z","iopub.status.idle":"2022-12-10T10:24:06.385367Z","shell.execute_reply.started":"2022-12-10T10:24:06.364068Z","shell.execute_reply":"2022-12-10T10:24:06.381572Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#folder = \"/kaggle/input/goodbadstyles/40_Styles/40_Styles\"\n#folders = [\"/kaggle/input/goodbadstyles/STYLES/Cathedral\", \n#           \"/kaggle/input/goodbadstyles/STYLES/Circuit_board\", \"/kaggle/input/goodbadstyles/STYLES/Soil\"]\nfolders = [\"/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data\"]\n\npredictions = []\nfor folder in folders:\n    predictions.append(predict_styles(clf, folder))","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:36:49.752728Z","iopub.execute_input":"2022-12-10T10:36:49.753100Z","iopub.status.idle":"2022-12-10T10:36:56.646647Z","shell.execute_reply.started":"2022-12-10T10:36:49.753071Z","shell.execute_reply":"2022-12-10T10:36:56.645030Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Cathedral_1.jpg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Cathedral_2.jpg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Cathedral_3.jpg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Cathedral_4.jpg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Cathedral_5.jpg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Circuit_Board_1.jpg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Circuit_Board_2.jpeg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Circuit_Board_3.jpg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Circuit_Board_4.jpg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Circuit_Board_5.jpeg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Soil_4.jpg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Soil_5.jpg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Soil_Grey.jpg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Soil_cracked.jpeg\n/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Soil_red.jpeg\n","output_type":"stream"}]},{"cell_type":"code","source":"#good_folders = [\"/kaggle/working/goodstyles/Cathedral\", \"/kaggle/working/goodstyles/Circuit_board\", \"/kaggle/working/goodstyles/Soil\"]\n#bad_folders = [\"/kaggle/working/badstyles/Cathedral\", \"/kaggle/working/badstyles/Circuit_board\", \"/kaggle/working/badstyles/Soil\"]\ngood_folders = [\"/kaggle/working/goodstyles\"]\nbad_folders = [\"/kaggle/working/badstyles\"]\nfor style_pred, good_style_folder, bad_style_folder in zip(predictions, good_folders, bad_folders):\n    os.makedirs(good_style_folder, exist_ok=True)\n    os.makedirs(bad_style_folder, exist_ok=True)\n    for pred in style_pred:\n        if pred[1]==1:\n            shutil.copy(pred[0], good_style_folder)\n        else:\n            shutil.copy(pred[0], bad_style_folder)","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:37:00.566143Z","iopub.execute_input":"2022-12-10T10:37:00.566556Z","iopub.status.idle":"2022-12-10T10:37:00.621114Z","shell.execute_reply.started":"2022-12-10T10:37:00.566509Z","shell.execute_reply":"2022-12-10T10:37:00.619569Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"shutil.make_archive(\"/kaggle/working/first_experiment_good\", 'zip', \"/kaggle/working/goodstyles\")\nshutil.make_archive(\"/kaggle/working/first_experiment_bad\", 'zip', \"/kaggle/working/badstyles\")\n","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:37:01.530259Z","iopub.execute_input":"2022-12-10T10:37:01.530779Z","iopub.status.idle":"2022-12-10T10:37:02.048260Z","shell.execute_reply.started":"2022-12-10T10:37:01.530744Z","shell.execute_reply":"2022-12-10T10:37:02.046392Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/first_experiment_bad.zip'"},"metadata":{}}]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2022-12-10T10:36:14.346192Z","iopub.execute_input":"2022-12-10T10:36:14.346652Z","iopub.status.idle":"2022-12-10T10:36:14.354586Z","shell.execute_reply.started":"2022-12-10T10:36:14.346619Z","shell.execute_reply":"2022-12-10T10:36:14.352933Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[[]]"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\n#PREVIOUS EXPERIMENTS\nfrom scipy import fft\nimport matplotlib.pyplot as plt\n# Example of fft\nimg = np.array(Image.open(\"/kaggle/input/goodbadstyles/GoodAndBadStyles/Good/AWA/Michael_2018-10-10_1715Z_cropped.jpg\").convert('L'))\nfourier = np.log(np.abs(fft.fftshift(fft.fft2(img, s=(20,20)))))\nplt.imshow(img)\nplt.show()\nplt.imshow(fourier)\n\"\"\"\n\"\"\"\n#from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.model_selection import cross_validate\n\nclassifiers = [SVC()]\n#svc is overfitting the best\n#, KNeighborsClassifier(), GaussianProcessClassifier(), DecisionTreeClassifier(), GaussianNB()]\n\nfolds=5\nfor clf in classifiers:\n    cv = cross_validate(clf, train_X, train_Y, cv=folds, scoring='balanced_accuracy')\n    #predictions = clf.predict(test_X)\n    #metric_performance = balanced_accuracy_score(test_Y, predictions)\n    print(cv['test_score'])\n    print(\"Average = %f\" %(sum(cv['test_score'])/folds))\n    \n\"\"\"\n\"\"\"\nexperiment 1: balanced accuracy, SVC model, resizing first to 128 then center cropping 128x128 resolution, first pool output (layer[6])\n[0.6875     1.         0.70833333 0.85714286 0.78571429]\nAverage = 0.807738\n\n>>BEST PERFORMER: experiment 2: same before, 256x256 resolution:\n[0.9375     0.85714286 0.6875     0.85714286 0.78571429]\nAverage = 0.825000\n\nexperiment 3: same as experiment 2, but taking first relu output (relu_1_1) instead\n[0.9375     0.79464286 0.6875     0.85714286 0.78571429]\nAverage = 0.812500\n\nexperiment 4: same as experiment 3, but taking relu_2_1 output instead\n[0.8125     0.85714286 0.625      0.85714286 0.78571429]\nAverage = 0.787500\n\nexperiment 5: same as experiment 4, but taking maxpool(2) output instead\n[0.75       1.         0.625      0.85714286 0.78571429]\nAverage = 0.803571\n\nexperiment 6: repeating experients 3, but with resolution 128 instead of 256\n[0.9375     0.79464286 0.6875     0.85714286 0.78571429]\nAverage = 0.812500\n\nexperiment 7: experiment 4, but 128x128\n[0.6875     0.85714286 0.625      0.85714286 0.78571429]\nAverage = 0.762500\n\nexperiment 8: experiment 5, but 128x128\n[0.6875     1.         0.70833333 0.85714286 0.78571429]\nAverage = 0.807738\n\nexperiment 9: resolution 256x256, taking output from third pooling layer\n[0.8125     0.86607143 0.70833333 0.78571429 0.85714286]\nAverage = 0.805952\n\"\"\"\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-12-06T20:42:01.369421Z","iopub.status.idle":"2022-12-06T20:42:01.370345Z","shell.execute_reply.started":"2022-12-06T20:42:01.370046Z","shell.execute_reply":"2022-12-06T20:42:01.370077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}