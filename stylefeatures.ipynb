{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-12-10T10:22:25.211451Z","iopub.status.busy":"2022-12-10T10:22:25.210812Z","iopub.status.idle":"2022-12-10T10:22:25.222897Z","shell.execute_reply":"2022-12-10T10:22:25.221305Z","shell.execute_reply.started":"2022-12-10T10:22:25.211414Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","import shutil\n","import glob\n","import torch\n","from torchvision.models import vgg19\n","from PIL import Image\n","from torchvision import transforms\n","from torchvision.models.feature_extraction import create_feature_extractor\n","import torch.nn.functional as F\n","\n","def image_loader(image_name):\n","    image = Image.open(image_name)\n","    # fake batch dimension required to fit network's input dimensions\n","    image = loader(image).unsqueeze(0)\n","    return image.to(device)\n","\n","normalization_mean = [0.485, 0.456, 0.406]\n","normalization_std = [0.229, 0.224, 0.225]\n","\n","resolution = (256,256)\n","\n","loader  = transforms.Compose([transforms.ToTensor(), \n","                                 transforms.Normalize(mean = normalization_mean, std = normalization_std),\n","                                 transforms.Resize(resolution[0]),\n","                                 transforms.CenterCrop(resolution)])\n","\n","device='cpu'"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:22:25.586771Z","iopub.status.busy":"2022-12-10T10:22:25.585895Z","iopub.status.idle":"2022-12-10T10:22:25.600386Z","shell.execute_reply":"2022-12-10T10:22:25.599313Z","shell.execute_reply.started":"2022-12-10T10:22:25.586720Z"},"trusted":true},"outputs":[],"source":["def store_styles(folder):\n","    \"\"\"\n","    Gets a list of styles and filenames from the folder (location)\n","    Param:\n","        folder: a location with format:\n","            >Bad\n","            >>images\n","            >Good\n","            >>class_1\n","            >>>images\n","            >>class_2\n","            >>>images\n","            ...\n","            where images = many files\n","    Returns:\n","        style_index: a list of styles\n","        result: a data dictionary where each filename is key, and the values is a list of 0/1 where 1 is the label for which the style is good\n","    \"\"\"\n","    \n","    conflict_exception = False\n","    \n","    result = dict()\n","\n","    good_folder = os.path.join(folder, \"Good\")\n","    #get all styles in the good folder\n","    styles = sorted(glob.glob(os.path.join(good_folder, \"*\")))\n","    \n","    #create list of styles which will help us associate a 1 at an index with a class\n","    #e.g. [1,1,1] is associated with good for \"ACT-410\", \"AWA\", \"INS\"\n","    #[0,0,1] is associated with good for \"INS\"\n","    #[0,0,0] is associated with bad style\n","    style_index = list()\n","    for style in styles:\n","        style_name = os.path.basename(style)\n","        style_index.append(style_name)\n","    \n","    #add good styles to our data\n","    for style in styles:\n","        style_name = os.path.basename(style)\n","        #get images in all styles:\n","        style_images = sorted(glob.glob(os.path.join(style, \"*\")))\n","        for style_image in style_images:\n","            \n","            #extract filename from path\n","            style_filename = os.path.basename(style_image)\n","            #if style filename is not already added to dictionary, create new dictionary entry\n","            if style_filename not in result:\n","                #create label list\n","                result[style_filename] = [0] * len(style_index)\n","            #set label of style index to 1\n","            result[style_filename][style_index.index(style_name)] += 1\n","    \n","    #add bad styles to our data\n","    bad_folder = os.path.join(folder, \"Bad\")\n","    #get all files in the folder of bad styles\n","    bad_images = sorted(glob.glob(os.path.join(bad_folder, \"*\")))\n","    \n","    for bad_image in bad_images:\n","        bad_style_filename = os.path.basename(bad_image)\n","        if bad_style_filename in result:\n","            print(\"Conflict found with image %s!\" %bad_style_filename)\n","            conflict_exception=True\n","            \n","        result[bad_style_filename] = [0] * len(style_index)\n","\n","    if conflict_exception:\n","        raise Exception(\"You can't have a bad style image which is also good!\")\n","        return \"BIG_ERROR\"\n","        \n","    return style_index, result"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:22:54.154942Z","iopub.status.busy":"2022-12-10T10:22:54.154442Z","iopub.status.idle":"2022-12-10T10:22:54.168582Z","shell.execute_reply":"2022-12-10T10:22:54.166693Z","shell.execute_reply.started":"2022-12-10T10:22:54.154908Z"},"trusted":true},"outputs":[],"source":["style_folder = \"/kaggle/input/goodbadstyles/KhuongsGoodAndBadStyles/GoodAndBadStyles\"\n","\n","style_index, result = store_styles(style_folder)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:22:54.651048Z","iopub.status.busy":"2022-12-10T10:22:54.650634Z","iopub.status.idle":"2022-12-10T10:22:54.658904Z","shell.execute_reply":"2022-12-10T10:22:54.657332Z","shell.execute_reply.started":"2022-12-10T10:22:54.651016Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0000218244_OG.jpeg\n","[1, 0, 0]\n"]}],"source":["for data in result:\n","    print(data)\n","    print(result[data])\n","    break"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:22:55.020161Z","iopub.status.busy":"2022-12-10T10:22:55.019350Z","iopub.status.idle":"2022-12-10T10:22:55.030377Z","shell.execute_reply":"2022-12-10T10:22:55.029364Z","shell.execute_reply.started":"2022-12-10T10:22:55.020125Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'0000218244_OG.jpeg': [1, 0, 0], '0719_Hurricane_Florence_1600.jpg': [1, 1, 1], '1024px-immortels_-_dynamosquito.jpg': [1, 0, 1], '121089-050-42fc73b7.jpg': [1, 0, 0], '14ac64752b04d2b7ba03023ad1309891.jpeg': [1, 0, 1], '1666527382925_tdy_sun_hurricane_roslyn_mexico_221023_1920x1080-87ukc2.jpg': [1, 1, 1], '220908145343-weather-satellite-hurricane-earl-20220908.jpg': [1, 1, 1], '3879022x.jpeg': [1, 0, 1], '6110392390_4ccdb68ec4_b.jpeg': [1, 0, 1], '8345-Babylon-lion.jpg': [1, 0, 1], '93f0869765e804ce24fb7a6638672dae--brick.jpg': [1, 1, 0], 'A-NASA-Joaquin.jpeg': [1, 1, 1], 'BwSgJO3IIAABWmE.jpeg': [1, 1, 1], 'Claude_Monet_Water_Lilies_1917-1919.jpeg': [1, 0, 1], 'EsgkveaXUAI1-sc.jpg': [1, 1, 1], 'IMG_3458.jpeg': [1, 0, 1], 'Michael_2018-10-10_1715Z_cropped.jpg': [1, 1, 1], 'Monet-04062015095820.jpeg': [1, 0, 0], 'autumn_rhythm-pollock1.jpg': [1, 0, 0], 'babylonian-dragon-01-left-weston-westmoreland.jpg': [1, 1, 0], 'colorful-autumn-leaves-background.jpg': [1, 0, 0], 'colorful-fabric-leaf-orange-red-by-Elizabeths-Studio-Landscape-Medley-208435-2.jpeg': [1, 1, 0], 'dsc09716-editar.jpg': [1, 1, 1], 'fa22d6dfbf0abb19a6c72f8fdbea2f1e.jpg': [1, 1, 0], 'fitow_tmo_2007247_lrg.jpg': [1, 0, 1], 'ian.7.jpg': [1, 0, 1], 'il_fullxfull525984995_d866.jpg': [1, 0, 1], 'image-asset.jpeg': [1, 0, 1], 'katewilson.jpg': [1, 0, 1], 'maxresdefault.jpg': [1, 1, 0], 'nortwhjgh4a91.jpeg': [1, 0, 0], 'packed-Autumn-leaf-fabric-by-Timeless-Treasures-232818-1.jpg': [1, 1, 1], 's-l1600.jpeg': [1, 0, 1], 'seawifs_olga_lrg.jpg': [1, 1, 1], 'skynews-florida-hurricane-nicole_5960570.jpg': [1, 0, 1], 'unnamed.jpeg': [1, 0, 1], 'water-lilies-red-claude-monet-1919-06ca30b4.jpeg': [1, 0, 1], '6881071-ZHWMOZLA-7.jpg': [0, 1, 0], 'Jackson_Pollock.jpg': [0, 1, 1], 'screen-6.jpeg': [0, 1, 0], 'secret-garden-flowers-natalie-holland.jpg': [0, 1, 0], 'madpoint-fun-floral-by-yaroslav-slavinskiy-wallpaper.png': [0, 0, 1], '0.jpg': [0, 0, 0], '1.jpg': [0, 0, 0], '10742886.jpg': [0, 0, 0], '10cd65a85382e93db319fcdf486a91b9--charcoal-sketch-lobe.jpg': [0, 0, 0], '122024114-red-flame-fire-texture-background.jpeg': [0, 0, 0], '13215095_dripping-5.jpg': [0, 0, 0], '14364437_style-of-jackson-pollock.jpg': [0, 0, 0], '2.jpeg': [0, 0, 0], '2797df19a8954d871fe31efbd8c8c4be.jpg': [0, 0, 0], '3.jpg': [0, 0, 0], '3b24418249188d57d76bab506549ceec.jpg': [0, 0, 0], '5cd7222fc34886942aef5c59f6c99825.jpg': [0, 0, 0], '6692775.jpg': [0, 0, 0], '684045.jpg': [0, 0, 0], '6c983fab1dd851e525638251218626dc.jpg': [0, 0, 0], '7.jpg': [0, 0, 0], '7932238_dripping.jpg': [0, 0, 0], '8880934_scream.jpg': [0, 0, 0], '93537.jpg': [0, 0, 0], 'DyxiOhdU0AAP-Wu.jpg': [0, 0, 0], 'EJ8sWvJWoAAP-L0.jpg': [0, 0, 0], 'Esn6jljXcAAltGE.jpg': [0, 0, 0], 'Flaming-Gold.jpg': [0, 0, 0], 'IMG_1874.JPG': [0, 0, 0], 'IMG_4022.jpg': [0, 0, 0], 'a106acf6786aee0b13e2ec4b5689e59a.jpg': [0, 0, 0], 'abstract-matrix-background-green-black-colors_108964-3.webp': [0, 0, 0], 'blaze-fire-flame.jpg': [0, 0, 0], 'code_matrix_numbers_200408_1920x1200.jpg': [0, 0, 0], 'depositphotos_15642823-stock-photo-seamless-fire-texture.jpg': [0, 0, 0], 'fall-maple-leaves-rustic-wooden-table_112977-2239.jpeg': [0, 0, 0], 'famous-jackson-pollock-paintings-1200x800.jpg': [0, 0, 0], 'fine-neon-green-matrix-xjxdv19fpcxfbord-xjxdv19fpcxfbord.jpg': [0, 0, 0], 'fire_texture1431.jpg': [0, 0, 0], 'gandhi-drip-painting.jpg': [0, 0, 0], 'il_fullxful394566304_pe6u.jpg': [0, 0, 0], 'images (1).jpeg': [0, 0, 0], 'images.jpeg': [0, 0, 0], 'jakson-pullock-16505422953x2.jpeg': [0, 0, 0], 'maple-leaves.jpg': [0, 0, 0], 'matrix-binary-code-green-d3.jpg': [0, 0, 0], 'matrix-code-numbers-green.jpg': [0, 0, 0], 'neon-green-honeycomb-matrix-6innruoaax2z3uxw-6innruoaax2z3uxw.jpg': [0, 0, 0], 'off-white-Timeless-Treasures-Autumn-maple-leaf-fabric-232819-2.jpg': [0, 0, 0], 'thumb-1920-77840.jpg': [0, 0, 0], 'vector-fire-flame-texture-background_88211-362.jpeg': [0, 0, 0], 'wp9929423.png': [0, 0, 0], 'wp9929428.jpg': [0, 0, 0]}\n","['ACT-410', 'AWA', 'INS']\n","[1, 0, 0]\n","{'000': 48, '001': 1, '010': 3, '011': 1, '100': 6, '101': 16, '110': 5, '111': 10}\n"]}],"source":["print(result)\n","print(style_index)\n","print(result[list(result.keys())[0]])\n","\n","#how many styles in each class\n","class_count = dict({\"000\" : 0, \"001\" : 0, \"010\" : 0, \"011\" : 0, \"100\" : 0, \"101\": 0, \"110\" : 0, \"111\" : 0})\n","\n","for data in result.values():\n","    class_ = str(data[0]) + str(data[1]) + str(data[2])\n","    class_count[class_] = class_count[class_] + 1\n","print(class_count)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:22:56.109251Z","iopub.status.busy":"2022-12-10T10:22:56.108560Z","iopub.status.idle":"2022-12-10T10:23:32.845939Z","shell.execute_reply":"2022-12-10T10:23:32.844654Z","shell.execute_reply.started":"2022-12-10T10:22:56.109210Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad6a91df62a6402090b9e5643d7d0b3b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["pretrained_vgg = vgg19(pretrained=True)\n","torch.set_grad_enabled(False)\n","layers = [(name,module) for name, module in pretrained_vgg.named_modules()]\n","return_nodes = {\n","            #experiment 1-2\n","            layers[6][0]: \"feature\"\n","            #experiment 3 #conv1-1\n","            #layers[3][0]: \"feature\"\n","            #experiment 4 #conv2-1\n","            #layers[8][0]: \"feature\"\n","            #experiment 5 #pooling 2\n","            #layers[11][0]: \"feature\"\n","            #experiment 9 #pooling 3\n","            #layers[20][0]: \"feature\"\n","        }\n","intermediate_vgg = create_feature_extractor(pretrained_vgg, return_nodes=return_nodes)\n","    \n","def process_1(file_location):   \n","    img = image_loader(file_location)\n","    features = intermediate_vgg(img)\n","    return features['feature']\n","\n","def process_2(file_location):\n","    \"\"\"placeholder function\"\"\"\n","    return np.random.rand(1)\n","\n","'''\n","def process_1(file_location):\n","    \"\"\"features about texture using fourier transform (FFT)\"\"\"\n","    img = np.array(Image.open(file_location).convert('L'))    \n","    return np.log(np.abs(fft.fftshift(fft.fft2(img, s=(20,20)))) + 10e-10).flatten()\n","\n","def process_2(file_location):\n","    \"\"\"features about histogram\"\"\"    \n","    img_rgb = np.array(Image.open(file_location).convert('RGB'))\n","    img_hsv = np.array(Image.open(file_location).convert('HSV'))      \n","    return np.concatenate([np.mean(img_rgb, axis=(0, 1)), np.std(img_rgb, axis=(0, 1)), np.mean(img_hsv, axis=(0, 1)), np.std(img_hsv, axis=(0, 1))])\n","'''\n","\n","def process_image(file_location):\n","    feature_0 = process_1(file_location)\n","    #feature_1 = process_2(file_location)\n","    return np.array(feature_0).flatten()\n","\n","def process_dataset(style_index, file_dict, filenames, data_path):\n","    \"\"\"\n","    Generates the train data for the given styles.\n","    Parameters:\n","        style_index: a list of styles to help identify which index of the list associated with a file belongs to a specific \"working style\";\n","                will be used to find the path of styles\n","        file_dict: a dictionary where the keys are all filenames and the values are a list which state the labels of the the filename\n","        filenames: a(n ordered) list of filenames, to ensure that the output will have the train data in the expected order\n","        data_path: path to the dataset in which folders [\"Bad\", \"Good\"] are found\n","    returns:\n","        X: a numpy array which is the train data for all the files in file_dict\n","    \"\"\"\n","    X = []\n","    for filename in filenames:\n","        #get list of classes for current filename\n","        current_labels = file_dict[filename]\n","        try:\n","            #get the location for the first class for which the style is good\n","            #due to structure of data, good styles are found in \"\n","            label_index = current_labels.index(1)\n","            \n","            style_class = style_index[label_index]\n","            \n","            current_class_location = os.path.join(data_path, \"Good\", style_class)\n","        except ValueError:\n","            #if not found, then the class is \"bad style\"\n","            current_class_location = os.path.join(data_path, \"Bad\")\n","        \n","        current_file_location = os.path.join(current_class_location, filename)\n","        X.append(process_image(current_file_location))\n","        \n","    return np.array(X)\n","        \n","        \n","    "]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:23:32.849209Z","iopub.status.busy":"2022-12-10T10:23:32.848640Z","iopub.status.idle":"2022-12-10T10:23:32.856756Z","shell.execute_reply":"2022-12-10T10:23:32.855310Z","shell.execute_reply.started":"2022-12-10T10:23:32.849169Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["('features.18', MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))\n"]}],"source":["print(layers[20])\n","#print(layers)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:23:32.859232Z","iopub.status.busy":"2022-12-10T10:23:32.858859Z","iopub.status.idle":"2022-12-10T10:23:33.620993Z","shell.execute_reply":"2022-12-10T10:23:33.619226Z","shell.execute_reply.started":"2022-12-10T10:23:32.859200Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["42\n","48\n"]}],"source":["negatives = 0\n","positives = 0\n","for key in result.keys():\n","    if sum(result[key])>0:\n","        positives+=1\n","    else:\n","        negatives+=1\n","        \n","print(positives)\n","print(negatives)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:23:33.625526Z","iopub.status.busy":"2022-12-10T10:23:33.624491Z","iopub.status.idle":"2022-12-10T10:23:56.800886Z","shell.execute_reply":"2022-12-10T10:23:56.799065Z","shell.execute_reply.started":"2022-12-10T10:23:33.625400Z"},"trusted":true},"outputs":[],"source":["#get filenames into a sorted list\n","filenames = sorted(list(result.keys()))\n","#gets data from the filenames\n","X = process_dataset(style_index, result, filenames, style_folder)\n","\n","Y = []\n","#get the true labels\n","for filename in filenames:\n","    #reminder: result is a dictionary where each filename has a label [int,int,int] associated with it\n","    #now we do binary classification so we only use one class\n","    if sum(result[filename])>0:\n","        Y.append(1)\n","    else:\n","        Y.append(0)\n","Y = np.array(Y)\n","\n","from sklearn.model_selection import train_test_split\n","#do the train/test split\n","train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:23:56.804151Z","iopub.status.busy":"2022-12-10T10:23:56.802687Z","iopub.status.idle":"2022-12-10T10:23:56.893273Z","shell.execute_reply":"2022-12-10T10:23:56.891666Z","shell.execute_reply.started":"2022-12-10T10:23:56.804086Z"},"trusted":true},"outputs":[],"source":["from sklearn.svm import SVC\n","from sklearn.metrics import balanced_accuracy_score"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:23:56.896420Z","iopub.status.busy":"2022-12-10T10:23:56.895636Z","iopub.status.idle":"2022-12-10T10:24:06.359656Z","shell.execute_reply":"2022-12-10T10:24:06.357988Z","shell.execute_reply.started":"2022-12-10T10:23:56.896383Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Balanced accuracy score = 0.887500\n"]}],"source":["clf = SVC()\n","clf.fit(train_X, train_Y)\n","preds = clf.predict(test_X)\n","metric = balanced_accuracy_score(test_Y, preds)\n","print(\"Balanced accuracy score = %f\" %metric)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:24:06.364144Z","iopub.status.busy":"2022-12-10T10:24:06.362238Z","iopub.status.idle":"2022-12-10T10:24:06.385367Z","shell.execute_reply":"2022-12-10T10:24:06.381572Z","shell.execute_reply.started":"2022-12-10T10:24:06.364068Z"},"trusted":true},"outputs":[],"source":["def predict_styles(clf, folder):\n","    \"\"\"\n","    Given a folder containing images, predict whether each style image is good or not.\n","    \"\"\"\n","    #get a list of images\n","    predictions = []\n","    images = sorted(glob.glob(os.path.join(folder, \"*\")))\n","    for image in images:\n","        print(image)\n","        X = process_image(image).reshape(1, -1)\n","        prediction = clf.predict(X)\n","        predictions.append([image, prediction])\n","    return predictions"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:36:49.753100Z","iopub.status.busy":"2022-12-10T10:36:49.752728Z","iopub.status.idle":"2022-12-10T10:36:56.646647Z","shell.execute_reply":"2022-12-10T10:36:56.645030Z","shell.execute_reply.started":"2022-12-10T10:36:49.753071Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Cathedral_1.jpg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Cathedral_2.jpg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Cathedral_3.jpg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Cathedral_4.jpg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Cathedral_5.jpg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Circuit_Board_1.jpg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Circuit_Board_2.jpeg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Circuit_Board_3.jpg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Circuit_Board_4.jpg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Circuit_Board_5.jpeg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Soil_4.jpg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Soil_5.jpg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Soil_Grey.jpg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Soil_cracked.jpeg\n","/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data/Soil_red.jpeg\n"]}],"source":["#folder = \"/kaggle/input/goodbadstyles/40_Styles/40_Styles\"\n","#folders = [\"/kaggle/input/goodbadstyles/STYLES/Cathedral\", \n","#           \"/kaggle/input/goodbadstyles/STYLES/Circuit_board\", \"/kaggle/input/goodbadstyles/STYLES/Soil\"]\n","folders = [\"/kaggle/input/goodbadstyles/Style_Dataset_jped/Style_Dataset/data\"]\n","\n","predictions = []\n","for folder in folders:\n","    predictions.append(predict_styles(clf, folder))"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:37:00.566556Z","iopub.status.busy":"2022-12-10T10:37:00.566143Z","iopub.status.idle":"2022-12-10T10:37:00.621114Z","shell.execute_reply":"2022-12-10T10:37:00.619569Z","shell.execute_reply.started":"2022-12-10T10:37:00.566509Z"},"trusted":true},"outputs":[],"source":["#good_folders = [\"/kaggle/working/goodstyles/Cathedral\", \"/kaggle/working/goodstyles/Circuit_board\", \"/kaggle/working/goodstyles/Soil\"]\n","#bad_folders = [\"/kaggle/working/badstyles/Cathedral\", \"/kaggle/working/badstyles/Circuit_board\", \"/kaggle/working/badstyles/Soil\"]\n","good_folders = [\"/kaggle/working/goodstyles\"]\n","bad_folders = [\"/kaggle/working/badstyles\"]\n","for style_pred, good_style_folder, bad_style_folder in zip(predictions, good_folders, bad_folders):\n","    os.makedirs(good_style_folder, exist_ok=True)\n","    os.makedirs(bad_style_folder, exist_ok=True)\n","    for pred in style_pred:\n","        if pred[1]==1:\n","            shutil.copy(pred[0], good_style_folder)\n","        else:\n","            shutil.copy(pred[0], bad_style_folder)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:37:01.530779Z","iopub.status.busy":"2022-12-10T10:37:01.530259Z","iopub.status.idle":"2022-12-10T10:37:02.048260Z","shell.execute_reply":"2022-12-10T10:37:02.046392Z","shell.execute_reply.started":"2022-12-10T10:37:01.530744Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/working/first_experiment_bad.zip'"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["good_folder = \"/kaggle/working/goodstyles\"\n","bad_folder = \"/kaggle/working/badstyles\"\n","good_archive = \"/kaggle/working/first_experiment_good\"\n","bad_archive = \"/kaggle/working/first_experiment_bad\"\n","\n","shutil.make_archive(good_archive, 'zip', good_folder)\n","shutil.make_archive(bad_archive, 'zip', bad_folder)\n"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-12-10T10:36:14.346652Z","iopub.status.busy":"2022-12-10T10:36:14.346192Z","iopub.status.idle":"2022-12-10T10:36:14.354586Z","shell.execute_reply":"2022-12-10T10:36:14.352933Z","shell.execute_reply.started":"2022-12-10T10:36:14.346619Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[[]]"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-12-06T20:42:01.369421Z","iopub.status.idle":"2022-12-06T20:42:01.370345Z","shell.execute_reply":"2022-12-06T20:42:01.370077Z","shell.execute_reply.started":"2022-12-06T20:42:01.370046Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","#PREVIOUS EXPERIMENTS\n","from scipy import fft\n","import matplotlib.pyplot as plt\n","# Example of fft\n","img = np.array(Image.open(\"/kaggle/input/goodbadstyles/GoodAndBadStyles/Good/AWA/Michael_2018-10-10_1715Z_cropped.jpg\").convert('L'))\n","fourier = np.log(np.abs(fft.fftshift(fft.fft2(img, s=(20,20)))))\n","plt.imshow(img)\n","plt.show()\n","plt.imshow(fourier)\n","\"\"\"\n","\"\"\"\n","#from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import balanced_accuracy_score\n","from sklearn.model_selection import cross_validate\n","\n","classifiers = [SVC()]\n","#svc is overfitting the best\n","#, KNeighborsClassifier(), GaussianProcessClassifier(), DecisionTreeClassifier(), GaussianNB()]\n","\n","folds=5\n","for clf in classifiers:\n","    cv = cross_validate(clf, train_X, train_Y, cv=folds, scoring='balanced_accuracy')\n","    #predictions = clf.predict(test_X)\n","    #metric_performance = balanced_accuracy_score(test_Y, predictions)\n","    print(cv['test_score'])\n","    print(\"Average = %f\" %(sum(cv['test_score'])/folds))\n","    \n","\"\"\"\n","\"\"\"\n","experiment 1: balanced accuracy, SVC model, resizing first to 128 then center cropping 128x128 resolution, first pool output (layer[6])\n","[0.6875     1.         0.70833333 0.85714286 0.78571429]\n","Average = 0.807738\n","\n",">>BEST PERFORMER: experiment 2: same before, 256x256 resolution:\n","[0.9375     0.85714286 0.6875     0.85714286 0.78571429]\n","Average = 0.825000\n","\n","experiment 3: same as experiment 2, but taking first relu output (relu_1_1) instead\n","[0.9375     0.79464286 0.6875     0.85714286 0.78571429]\n","Average = 0.812500\n","\n","experiment 4: same as experiment 3, but taking relu_2_1 output instead\n","[0.8125     0.85714286 0.625      0.85714286 0.78571429]\n","Average = 0.787500\n","\n","experiment 5: same as experiment 4, but taking maxpool(2) output instead\n","[0.75       1.         0.625      0.85714286 0.78571429]\n","Average = 0.803571\n","\n","experiment 6: repeating experients 3, but with resolution 128 instead of 256\n","[0.9375     0.79464286 0.6875     0.85714286 0.78571429]\n","Average = 0.812500\n","\n","experiment 7: experiment 4, but 128x128\n","[0.6875     0.85714286 0.625      0.85714286 0.78571429]\n","Average = 0.762500\n","\n","experiment 8: experiment 5, but 128x128\n","[0.6875     1.         0.70833333 0.85714286 0.78571429]\n","Average = 0.807738\n","\n","experiment 9: resolution 256x256, taking output from third pooling layer\n","[0.8125     0.86607143 0.70833333 0.78571429 0.85714286]\n","Average = 0.805952\n","\"\"\"\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
