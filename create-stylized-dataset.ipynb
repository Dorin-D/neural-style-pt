{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport neural_style\nimport download_models\nfrom PIL import Image\nimport shutil","metadata":{"_uuid":"4beba73e-ae1f-414b-ba30-586c58e4f37b","_cell_guid":"de91546e-6bf0-4102-8330-cd9c473fa423","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-12T20:24:19.328683Z","iopub.execute_input":"2022-12-12T20:24:19.329475Z","iopub.status.idle":"2022-12-12T20:24:21.756329Z","shell.execute_reply.started":"2022-12-12T20:24:19.329374Z","shell.execute_reply":"2022-12-12T20:24:21.754238Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def choose_classes(labels, n_classes, n_samples):\n    \"\"\"\n    Chooses n_classes that contain at least n_train, n_test : train,test samples from the given labels dataframe.\n    parameters:\n        labels: pandas dataframe containing filenames, labels and train/test split\n        n_classes: required number of classes\n        n_samples: required number of samples in a class\n    returns:\n        chosen_classes: an array of the names of the classes chosen    \n    \"\"\"\n    #number of classes\n    total_classes = labels['CATEGORY'].nunique()\n    #array of classes\n    classes = labels.CATEGORY.unique()\n\n    assert total_classes >= n_classes, \"n_classes must be smaller than the number of available classes. (choose_classes function)\"\n\n    #create random permutation of n_classes from all classes\n    class_random_sampling = np.arange(total_classes)\n    class_random_sampling = np.random.permutation(class_random_sampling)\n    \n    samples_per_class = labels.groupby(['CATEGORY'])['ORIG_CATEGORY_FILENAME'].count().reset_index(name='count')\n    chosen_classes = []\n    for c in class_random_sampling:\n        samples_per_current_class = samples_per_class.loc[samples_per_class['CATEGORY']==classes[c]]\n        #get amount of rows in this class, as INTEGER instead of SERIES >.<\n        samples_count_current_class = samples_per_current_class['count'].iloc[0]\n        if (samples_count_current_class < n_samples):\n            #if not enough samples, we ignore this class and take the next one\n            continue\n        chosen_classes.append(classes[c])\n        \n        if len(chosen_classes) == n_classes:\n            break\n\n    return chosen_classes\n\ndef choose_styles(labels, n_styles):\n    \"\"\"\n    Chooses n_styles randomly from the labels dataframe. \n    parameters:\n        labels: pandas dataframe containing filenames, style labels and train/test split of each file\n        n_styles: number of styles to choose\n    returns:\n        chosen_styles: an array of the names of the styles chosen\n    \"\"\"\n    #number of style classes\n    total_classes = labels['STYLE'].nunique()\n    #array of classes\n    classes = labels.STYLE.unique()\n    \n    assert total_classes >= n_styles, \"n_classes must be smaller than the number of available classes. (choose_styles function)\"\n    \n    #create random permutation of n_classes from all classes\n    class_random_sampling = np.arange(total_classes)\n    class_random_sampling = np.random.permutation(class_random_sampling)\n\n    chosen_styles = []\n    for s in class_random_sampling:\n        chosen_styles.append(classes[s])\n        if chosen_styles == n_styles:\n            break\n    return chosen_styles\n \ndef choose_style_image(style_location, style):\n    \"\"\"\n    Given a style, finds its associated images using the label file in style_location and returns a random filename of the style.\n    parameters:\n        style_location: location containing the folder with style images and the label.csv\n        style: name of the style\n    returns:\n        chosen_image: location of the chosen style image\n    \"\"\"\n    \n    label = os.path.join(style_location, \"labels.csv\")\n    images_location = os.path.join(style_location, \"data\")\n\n    label_csv = pd.read_csv(label)\n    label_csv.rename(columns = {\"FILENAME\" : \"ORIG_STYLE_FILENAME\"}, inplace=True)\n    \n    #get all filenames of images of said style\n    style_images = label_csv.loc[label_csv['STYLE']==style]['ORIG_STYLE_FILENAME']\n    #choose a random image from this list\n    random_index = int(len(style_images) * np.random.rand(1))\n    chosen_image = style_images.iloc[random_index]\n    return chosen_image","metadata":{"execution":{"iopub.status.busy":"2022-12-12T20:33:03.475927Z","iopub.execute_input":"2022-12-12T20:33:03.476542Z","iopub.status.idle":"2022-12-12T20:33:03.524491Z","shell.execute_reply.started":"2022-12-12T20:33:03.476491Z","shell.execute_reply":"2022-12-12T20:33:03.522564Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def create_stylized_dataset(location, location_styles, n_classes, n_styles, \n                            n_samples,\n                            output_location,\n        #the rest are remaining neural style transfer arguments\n        p_style_weight=\"1e2\", p_content_weight=\"5e0\", p_num_iterations=\"1000\", p_learning_rate = \"1e0\", \n        p_gpu=\"0\", p_image_size=\"512\", p_style_blend_weights=\"None\", p_normalize_weights=\"False\", p_normalize_gradients=\"False\", p_tv_weight=\"1e-3\", p_init='random', p_init_image=\"None\", p_optimizer='lbfgs', \n        p_lbfgs_num_correction=\"100\",\n        p_print_iter=\"0\", p_save_iter=\"0\", p_style_scale=\"1.0\", p_original_colors = \"0\", p_model_file='models/vgg19-d01eb7cb.pth', p_disable_check=\"False\",\n        p_backend='nn', p_cudnn_autotune=\"False\", p_pooling='max',\n        p_seed=\"-1\", p_content_layers='relu4_2', p_style_layers='relu1_1,relu2_1,relu3_1,relu4_1,relu5_1', p_multidevice_strategy='4,7,29'):\n    \"\"\"\n    Given a domain, applies style transfer to n_samples per each class,style group.\n    Generates a .csv file which contains the filenames, the classes, and the assigned style to them.\n    \n    parameters:\n        location: the location(s) of the domain(s) to be used; if multiple domains given, choose randomly;\n                format of domain folders:\n                    location_folder\n                        >data\n                            >>all images will be in this folder\n                        >labels.csv\n                            >>this file will contain the columns: filename, label, split\n                                >>filename: name of the file\n                                >>label: name of the class\n                                N/A>>split: test or train; specifying the split of the sample\n        location_styles: the location with the styles to be used; format:\n            location_styles\n                >data\n                    >>all style images will be in this folder\n                >labels.csv\n                    >>this file will contain the columns: filename, label\n                        >>filename: name of the style file\n                        >>label: name of the style\n                        N/A>>split: which split the style is in\n\n        n_classes: amount of classes from the dataset to apply styles to\n        n_styles: amount of styles to apply\n            (n_classes, n_styles) should be equal?\n        n_samples: number of samples per class\n        output_location: location where to output the label, data\n    output:\n        tbd\n    \"\"\"\n    if type(location)==str:\n        pass\n    elif type(location) in (list,tuple):\n        location = random.choice(location)\n    else:\n        #you can't have neither a list, tuple nor a str!!!!\n        raise Exception(\"Please don't do this to me (╥﹏╥)\")\n    \n    data = os.path.join(location, \"data\")\n    label_loc = os.path.join(location, \"labels.csv\")\n    labels = pd.read_csv(label_loc)\n    \n    #rename filename to orig category name\n    labels.rename(columns = {\"FILENAME\" : \"ORIG_CATEGORY_FILENAME\"}, inplace=True)\n    \n    \n    n_samples_per_class = n_styles*n_samples #e.g. 40 samples per group, 3 styles means we need 120 samples for a class\n    \n    chosen_classes = choose_classes(labels, n_classes, n_samples_per_class)\n    assert len(chosen_classes) == n_classes, \"Likely there aren't enough classes to have at least n_samples samples (too many or too little classes were chosen)\"\n\n    style_label_loc = os.path.join(location_styles, \"labels.csv\")\n    style_labels = pd.read_csv(style_label_loc)    \n    \n    style_labels.rename(columns = {\"FILENAME\" : \"ORIG_STYLE_FILENAME\"}, inplace=True)\n    \n    chosen_styles = choose_styles(style_labels, n_styles)    \n    assert len(chosen_styles) == n_styles, \"Likely there aren't enough styles offered in the style label.csv file\"\n\n    #the assumption is that n_classes, n_styles are equal\n    \n    groups_style_class = []\n    for c in chosen_classes:\n        for s in chosen_styles:\n            groups_style_class.append([c,s])\n\n    labels_wstyles = pd.DataFrame()\n\n\n    for g_s_c in groups_style_class:\n        #select the class from the g_s_c pair\n        c = g_s_c[0]\n        #select samples from current class c\n        samples = labels[labels['CATEGORY']==c].reset_index(drop=True)\n\n        #get permutations for train,test samples\n        random_samples_permutation = np.random.permutation(np.arange(len(samples)))\n\n\n        #select first n_samples samples from permutation\n        chosen_samples = samples.iloc[random_samples_permutation[0:n_samples]][['ORIG_CATEGORY_FILENAME', 'CATEGORY']]\n        #reset indices of chosen samples\n        chosen_samples = chosen_samples.reset_index(drop=True)\n\n        #get permutations for group samples\n        chosen_samples_permutation = np.random.permutation(np.arange(len(chosen_samples)))\n        \n        chosen_style = g_s_c[1]\n        \n        chosen_images = chosen_samples_permutation[:n_samples]\n        chosen_samples_permutation = chosen_samples_permutation[n_samples:]\n        chosen_samples.loc[chosen_images, 'STYLE'] = chosen_style\n\n        labels_wstyles = pd.concat((labels_wstyles, chosen_samples), ignore_index=True)\n\n    #for all entries, choose a style image of given style\n    style_location_list = []\n    for index,row in labels_wstyles.iterrows():\n        style_location_list.append(choose_style_image(location_styles, row['STYLE']))\n\n    style_location_df = pd.DataFrame(style_location_list, columns=['ORIG_STYLE_FILENAME'])\n    labels_wstyles['ORIG_STYLE_FILENAME'] = style_location_df\n    \n    #TODO: add neural style transferoutput_location\n    #TODO: formalize output\n    output_label = os.path.join(output_location, 'label.csv')\n    output_data = os.path.join(output_location, \"data\")\n\n    #create folders necessary for output_data (and output_label)\n    os.makedirs(output_data, exist_ok=True)\n    \n    labels_wstyles['FILENAME'] = labels_wstyles['ORIG_CATEGORY_FILENAME'] + \"_\" + labels_wstyles['ORIG_STYLE_FILENAME']\n    labels_wstyles.to_csv(output_label)\n\n    #create stylised dataset\n    for index,row in labels_wstyles.iterrows():\n        location_style_image = os.path.join(location_styles, \"data\", row['ORIG_STYLE_FILENAME'])\n        location_content_image = os.path.join(location, \"data\", row['ORIG_CATEGORY_FILENAME'])\n        location_output_image = os.path.join(output_location, \"data\", row['FILENAME'])\n        \n        with Image.open(location_content_image) as img:\n            width, height = img.size\n            #choosing the smaller value between image size, and the requested p_image_size\n            #target_image_size = int(max(width, height))\n            target_image_size = min(p_image_size, int(max(width, height)))\n            \n        \n        command = \"/kaggle/usr/lib/neural_style/neural_style.py -style_image %s -style_blend_weights %s -content_image %s -image_size %s -gpu %s -content_weight %s -style_weight %s normalize_weights %s -normalize_gradients %s -tv_weight %s -num_iterations %s -init %s -init_image %s -optimizer %s -learning_rate %s -lbfgs_num_correction %s -print_iter %s -save_iter %s -output_image %s -style_scale %s -original_colors %s -pooling %s -model_file %s -disable_check %s -backend %s -cudnn_autotune %s -seed %s -content_layers %s -style_layers %s -multidevice_strategy %s\" %(\n                location_style_image, p_style_blend_weights, location_content_image, target_image_size, p_gpu, p_content_weight, p_style_weight, p_normalize_weights, p_normalize_gradients, p_tv_weight, p_num_iterations, p_init, p_init_image, p_optimizer, p_learning_rate, p_lbfgs_num_correction, p_print_iter, p_save_iter, location_output_image, p_style_scale, p_original_colors, p_pooling, p_model_file, p_disable_check, p_backend, p_cudnn_autotune, p_seed, p_content_layers, p_style_layers, p_multidevice_strategy)\n        !python3 $command\n        print(\"Finished running: %s\" %command)\n                    \n    return 0","metadata":{"execution":{"iopub.status.busy":"2022-12-12T20:33:03.739848Z","iopub.execute_input":"2022-12-12T20:33:03.740465Z","iopub.status.idle":"2022-12-12T20:33:05.021900Z","shell.execute_reply.started":"2022-12-12T20:33:03.740396Z","shell.execute_reply":"2022-12-12T20:33:05.020760Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"p_model_location = \"/kaggle/working/models\"\ndownload_models.main(p_model_location)\np_model_file = os.path.join(p_model_location, \"vgg19-d01eb7cb.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-12-12T20:33:05.024272Z","iopub.execute_input":"2022-12-12T20:33:05.024641Z","iopub.status.idle":"2022-12-12T20:33:05.038950Z","shell.execute_reply.started":"2022-12-12T20:33:05.024606Z","shell.execute_reply":"2022-12-12T20:33:05.037918Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"All models have been successfully downloaded\n","output_type":"stream"}]},{"cell_type":"code","source":"location = \"/kaggle/input/ter-set-1/Human_Actions/Human_Actions\"\nlocation_styles = \"/kaggle/input/ter-set-1/Classified_Style_Dataset/Classified_Style_Dataset\"\nn_classes = 3\nn_styles = 3\nn_samples = 40\n\noutput_location = \"/kaggle/working/output/Human_Actions_Stylized_Experiment_12122022\"\n\ncreate_stylized_dataset(location, location_styles, n_classes, n_styles, n_samples, output_location,\n                        p_model_file = p_model_file, p_original_colors = \"1\", p_style_weight=25, p_image_size=512\n        #the rest are remaining neural style transfer arguments\n                       )\n\"\"\"\n        ,p_style_weight=1e2, p_content_weight=5e0, p_num_iterations=1000, p_learning_rate = 1e0, \n        p_gpu=0, p_image_size=512, p_style_blend_weights=None, p_normalize_weights=False, p_normalize_gradients=False, p_tv_weight=1e-3, p_init='random', p_init_image=None, p_optimizer='lbfgs', \n        p_lbfgs_num_correction=100,\n        p_print_iter=0, p_save_iter=0, p_style_scale=1.0, p_original_colors = 0, p_model_file='models/vgg19-d01eb7cb.pth', p_disable_check=False, \n        p_backend='nn', p_cudnn_autotune=False, p_pooling='max',\n        p_seed=-1, p_content_layers='relu4_2', p_style_layers='relu1_1,relu2_1,relu3_1,relu4_1,relu5_1', p_multidevice_strategy='4,7,29')\n\"\"\"\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-12-12T20:33:05.040761Z","iopub.execute_input":"2022-12-12T20:33:05.041187Z","iopub.status.idle":"2022-12-12T20:44:37.595767Z","shell.execute_reply.started":"2022-12-12T20:33:05.041148Z","shell.execute_reply":"2022-12-12T20:44:37.594104Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"All models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Classified_Style_Dataset/Classified_Style_Dataset/data/Soil_12.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/Human_Actions/Human_Actions/data/riding_a_bike_253.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment_12122022/data/riding_a_bike_253.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Classified_Style_Dataset/Classified_Style_Dataset/data/Circuit_Board_10.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/Human_Actions/Human_Actions/data/riding_a_bike_271.jpg -image_size 450 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment_12122022/data/riding_a_bike_271.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Classified_Style_Dataset/Classified_Style_Dataset/data/Cathedral_10.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/Human_Actions/Human_Actions/data/riding_a_bike_143.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment_12122022/data/riding_a_bike_143.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Classified_Style_Dataset/Classified_Style_Dataset/data/Soil_12.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/Human_Actions/Human_Actions/data/pushing_a_cart_051.jpg -image_size 400 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment_12122022/data/pushing_a_cart_051.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Classified_Style_Dataset/Classified_Style_Dataset/data/Circuit_Board_12.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/Human_Actions/Human_Actions/data/pushing_a_cart_071.jpg -image_size 400 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment_12122022/data/pushing_a_cart_071.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Classified_Style_Dataset/Classified_Style_Dataset/data/Cathedral_3.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/Human_Actions/Human_Actions/data/pushing_a_cart_016.jpg -image_size 272 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment_12122022/data/pushing_a_cart_016.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Classified_Style_Dataset/Classified_Style_Dataset/data/Soil_9.jpeg -style_blend_weights None -content_image /kaggle/input/ter-set-1/Human_Actions/Human_Actions/data/playing_guitar_013.jpg -image_size 451 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment_12122022/data/playing_guitar_013.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Classified_Style_Dataset/Classified_Style_Dataset/data/Circuit_Board_14.jpeg -style_blend_weights None -content_image /kaggle/input/ter-set-1/Human_Actions/Human_Actions/data/playing_guitar_037.jpg -image_size 300 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment_12122022/data/playing_guitar_037.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Classified_Style_Dataset/Classified_Style_Dataset/data/Cathedral_4.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/Human_Actions/Human_Actions/data/playing_guitar_156.jpg -image_size 447 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment_12122022/data/playing_guitar_156.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#shutil.rmtree(\"/kaggle/working/output/\")\n#os.remove(\"/kaggle/working/output_archive.zip\")\n#os.makedirs(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2022-12-12T20:32:59.100767Z","iopub.execute_input":"2022-12-12T20:32:59.105834Z","iopub.status.idle":"2022-12-12T20:32:59.115444Z","shell.execute_reply.started":"2022-12-12T20:32:59.105762Z","shell.execute_reply":"2022-12-12T20:32:59.114454Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/output_archive\", 'zip', \"/kaggle/working/output\")","metadata":{"execution":{"iopub.status.busy":"2022-12-12T20:47:54.094505Z","iopub.execute_input":"2022-12-12T20:47:54.094905Z","iopub.status.idle":"2022-12-12T20:47:54.123795Z","shell.execute_reply.started":"2022-12-12T20:47:54.094869Z","shell.execute_reply":"2022-12-12T20:47:54.122489Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/output_archive.zip'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-12-07T11:06:43.591638Z","iopub.execute_input":"2022-12-07T11:06:43.592001Z","iopub.status.idle":"2022-12-07T11:06:43.600873Z","shell.execute_reply.started":"2022-12-07T11:06:43.591963Z","shell.execute_reply":"2022-12-07T11:06:43.599685Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"52"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}