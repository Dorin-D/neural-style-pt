{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport neural_style\nimport download_models\nfrom PIL import Image\nimport shutil","metadata":{"_uuid":"4beba73e-ae1f-414b-ba30-586c58e4f37b","_cell_guid":"de91546e-6bf0-4102-8330-cd9c473fa423","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-07T08:35:22.361657Z","iopub.execute_input":"2022-12-07T08:35:22.362077Z","iopub.status.idle":"2022-12-07T08:35:24.337127Z","shell.execute_reply.started":"2022-12-07T08:35:22.361996Z","shell.execute_reply":"2022-12-07T08:35:24.335838Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def choose_classes(labels, n_classes, n_train, n_test):\n    \"\"\"\n    Chooses n_classes that contain at least n_train, n_test : train,test samples from the given labels dataframe.\n    parameters:\n        labels: pandas dataframe containing filenames, labels and train/test split\n        n_classes: required number of classes\n        n_train: required number of train samples\n        n_test: required number of test samples\n    returns:\n        chosen_classes: an array of the names of the classes chosen    \n    \"\"\"\n    #number of classes\n    total_classes = labels['label'].nunique()\n    #array of classes\n    classes = labels.label.unique()\n\n    assert total_classes >= n_classes, \"n_classes must be smaller than the number of available classes. (choose_classes function)\"\n\n    #create random permutation of n_classes from all classes\n    class_random_sampling = np.arange(total_classes)\n    class_random_sampling = np.random.permutation(class_random_sampling)\n    \n    #count samples in each class,split\n    samples_per_class = labels.groupby(['label','split'])['filename'].count().reset_index(name='count')\n    chosen_classes = []\n    for c in class_random_sampling:\n        #get amount of samples per class for both splits\n        samples_per_current_class = samples_per_class.loc[samples_per_class['label']==classes[c]]\n        #get amount of samples per split\n        train_samples_per_class = samples_per_current_class.loc[samples_per_current_class['split']=='train']['count'].reset_index(drop=True)[0]\n        test_samples_per_class = samples_per_current_class.loc[samples_per_current_class['split']=='test']['count'].reset_index(drop=True)[0]\n        if (train_samples_per_class < n_train) or (test_samples_per_class < n_test):\n            #if not enough samples, we ignore this class and take the next one\n            continue\n        chosen_classes.append(classes[c])\n        if len(chosen_classes) == n_classes:\n            break\n\n    return chosen_classes\n\ndef choose_styles(labels, n_styles):\n    \"\"\"\n    Chooses n_styles randomly from the labels dataframe. \n    parameters:\n        labels: pandas dataframe containing filenames, style labels and train/test split of each file\n        n_styles: number of styles to choose\n    returns:\n        chosen_styles: an array of the names of the styles chosen\n    \"\"\"\n    #number of style classes\n    total_classes = labels['label'].nunique()\n    #array of classes\n    classes = labels.label.unique()\n    \n    assert total_classes >= n_styles, \"n_classes must be smaller than the number of available classes. (choose_styles function)\"\n    \n    #create random permutation of n_classes from all classes\n    class_random_sampling = np.arange(total_classes)\n    class_random_sampling = np.random.permutation(class_random_sampling)\n\n    chosen_styles = []\n    for s in class_random_sampling:\n        chosen_styles.append(classes[s])\n        if chosen_styles == n_styles:\n            break\n    return chosen_styles\n \ndef choose_style_image(style_location, style, split):\n    \"\"\"\n    Given a style and split, finds its associated images using the label file and split in style_location and returns a random filename of the style.\n    parameters:\n        style_location: location containing the folder with style images and the label.csv\n        style: name of the style\n        split: whether it's a train or test style image\n    returns:\n        chosen_image: location of the chosen style image\n    \"\"\"\n\n    label = os.path.join(style_location, \"labels.csv\")\n    images_location = os.path.join(style_location, \"data\")\n\n    label_csv = pd.read_csv(label)\n    \n    #get all filenames and splits of images of said style\n    style_images = label_csv.loc[label_csv['label']==style][['filename', 'split']]\n    #get all filenames of given split from previously selected style\n    style_split_images = style_images.loc[style_images['split']==split]['filename']\n    #choose a random image from this list\n    random_index = int(len(style_split_images) * np.random.rand(1))\n    chosen_image = style_split_images.iloc[random_index]\n    return chosen_image","metadata":{"execution":{"iopub.status.busy":"2022-12-07T08:35:26.772443Z","iopub.execute_input":"2022-12-07T08:35:26.772931Z","iopub.status.idle":"2022-12-07T08:35:26.787239Z","shell.execute_reply.started":"2022-12-07T08:35:26.772897Z","shell.execute_reply":"2022-12-07T08:35:26.786275Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def create_stylized_dataset(location, location_styles, n_classes, n_styles, n_train, n_test, p_train, output_location,\n        #the rest are remaining neural style transfer arguments\n        p_style_weight=\"1e2\", p_content_weight=\"5e0\", p_num_iterations=\"1000\", p_learning_rate = \"1e0\", \n        p_gpu=\"0\", p_image_size=\"512\", p_style_blend_weights=\"None\", p_normalize_weights=\"False\", p_normalize_gradients=\"False\", p_tv_weight=\"1e-3\", p_init='random', p_init_image=\"None\", p_optimizer='lbfgs', \n        p_lbfgs_num_correction=\"100\",\n        p_print_iter=\"0\", p_save_iter=\"0\", p_style_scale=\"1.0\", p_original_colors = \"0\", p_model_file='models/vgg19-d01eb7cb.pth', p_disable_check=\"False\",\n        p_backend='nn', p_cudnn_autotune=\"False\", p_pooling='max',\n        p_seed=\"-1\", p_content_layers='relu4_2', p_style_layers='relu1_1,relu2_1,relu3_1,relu4_1,relu5_1', p_multidevice_strategy='4,7,29'):\n    \"\"\"\n    Given a domain, extracts (n_train, n_test) samples from n_classes. It assigns a dominant style to each class. For the class, \n        it assigns p_train*n_train (rounded down) samples to the dominant style, and (1-p_train)/(n_styles-1) (rounded down) samples to the non-dominant styles.\n        Samples which are left out (due to rounding down) are assigned the dominant style. \n        Generates a .csv file which contains the filenames, the classes, the splits and the assigned style to them.\n        TODO: Apply neural style transfer and create the actual datasets.\n    parameters:\n        location: the location(s) of the domain(s) to be used; if multiple domains given, choose randomly;\n                format of domain folders:\n                    location_folder\n                        >data\n                            >>all images will be in this folder\n                        >labels.csv\n                            >>this file will contain the columns: filename, label, split\n                                >>filename: name of the file\n                                >>label: name of the class\n                                >>split: test or train; specifying the split of the sample\n        location_styles: the location with the styles to be used; format:\n            location_styles\n                >data\n                    >>all style images will be in this folder\n                >labels.csv\n                    >>this file will contain the columns: filename, label\n                        >>filename: name of the style file\n                        >>label: name of the style\n                        >>split: which split the style is in\n\n        n_classes: amount of classes from the dataset to apply styles to\n        n_styles: amount of styles to apply\n            (n_classes, n_styles) should be equal?\n        n_train: amount of train samples per class\n        n_test: amount of test samples per class\n            (n_train, n_test) should be equal?\n        p_train: amount of bias in train set (p=0.9 => 90% of images will be of dominant class)\n        output_location: location where to output the label, data\n    output:\n        tbd\n    \"\"\"\n    if type(location)==str:\n        pass\n    elif type(location) in (list,tuple):\n        location = random.choice(location)\n    else:\n        #you can't have neither a list, tuple nor a str!!!!\n        raise Exception(\"Please don't do this to me (╥﹏╥)\")\n    \n    data = os.path.join(location, \"data\")\n    label_loc = os.path.join(location, \"labels.csv\")\n    labels = pd.read_csv(label_loc)\n\n    \n    chosen_classes = choose_classes(labels, n_classes, n_train, n_test)\n    assert len(chosen_classes) == n_classes, \"Likely there aren't enough classes to have at least n_train,n_test samples (too many or too little classes were chosen)\"\n\n    style_label_loc = os.path.join(location_styles, \"labels.csv\")\n    style_labels = pd.read_csv(style_label_loc)    \n    chosen_styles = choose_styles(style_labels, n_styles)    \n    assert len(chosen_styles) == n_styles, \"Likely there aren't enough styles offered in the style label.csv file\"\n\n    #dominant ratio: the ratio of train samples in the dominant style\n    #converting ratio to percentage\n    dominant_ratio = 100*p_train/100\n    #non dominant ratio: the ratio of train samples in the non-dominant styles\n    #converting ratio to percentage 100 is used to fix bug where 1-0.9=0.499999999999999999 -.-\n    non_dominant_ratio = (100-100*p_train)/(n_styles-1)/100\n    \n    #amount of images in dominant and non-dominant styles\n    images_dominant = int(n_train * dominant_ratio)\n    images_non_dominant = int(n_train * non_dominant_ratio)\n    \n    images_test_set = int(n_test * (1/n_styles))\n    #assert images_test_set * n_styles == n_test, \n    #                \"Proportions not worked out for the test set; pick values which result in whole numbers, please\"\n    \n    #code doesn't work if ratios result in non-integer numbers; could be improved\n    #update; code has been improved by adding leftover images to the dominant class\n    #assert (images_dominant + (n_styles-1) * images_non_dominant) == n_train, \"Proportions not worked out; pick values which result in whole numbers\"\n\n    #the assumption is that n_classes, n_styles are equal\n    #pairs each class with a style which will be dominant\n    dominant_style_class = []\n    for c,s in zip(chosen_classes, chosen_styles):\n        dominant_style_class.append([c,s])\n\n    labels_wstyles = pd.DataFrame()\n\n\n    for d_s_c in dominant_style_class:\n        #select the class from the d_s_c pair\n        c = d_s_c[0]\n        #select samples from current class\n        samples = labels[labels['label']==c].reset_index(drop=True)\n        #select train samples for current class c\n        train_samples = samples[samples['split']=='train'].reset_index(drop=True)\n        #select test samples for current class c\n        test_samples = samples[samples['split']=='test'].reset_index(drop=True)\n\n        #get permutations for train,test samples\n        random_train_permutation = np.random.permutation(np.arange(len(train_samples)))\n        random_test_permutation = np.random.permutation(np.arange(len(test_samples)))\n\n\n        #select first n_train, n_test samples from permutation\n        chosen_trains = train_samples.iloc[random_train_permutation[0:n_train]][['filename', 'label', 'split']]\n        chosen_tests = test_samples.iloc[random_test_permutation[0:n_test]][['filename', 'label', 'split']]\n        #reset indices of chosen samples\n        chosen_trains = chosen_trains.reset_index(drop=True)\n        chosen_tests = chosen_tests.reset_index(drop=True)\n\n        #get permutations for train,test samples\n        chosen_train_permutation = np.random.permutation(np.arange(len(chosen_trains)))\n        chosen_test_permutation = np.random.permutation(np.arange(len(chosen_tests)))\n\n        #assign styles to train set\n        for s in chosen_styles:\n            if d_s_c[1]==s:\n                #we have the dominant style\n                #select first images_dominant to be of dominant styles, the remaining will be left for the other styles\n                chosen_images = chosen_train_permutation[:images_dominant]\n                chosen_train_permutation = chosen_train_permutation[images_dominant:]\n                chosen_trains.loc[chosen_images, 'style'] = s\n\n            else:\n                #we have non dominant style\n                #select first images_non_dominant to be of non dominant styles, the remaining will be left for the other styles\n                chosen_images = chosen_train_permutation[:images_non_dominant]\n                chosen_train_permutation = chosen_train_permutation[images_non_dominant:]\n                chosen_trains.loc[chosen_images, 'style'] = s\n\n        if len(chosen_train_permutation)>0:\n            #if there's leftover images, assign them to dominant class\n            chosen_trains.loc[chosen_train_permutation, 'style'] = d_s_c[1]\n        #assign styles to test set\n        for s in chosen_styles:\n            #we have non dominant style\n            #select first images_test_set to be of non dominant styles, the remaining will be left for the other styles\n            chosen_images = chosen_test_permutation[:images_test_set]\n            chosen_test_permutation = chosen_test_permutation[images_test_set:]\n            chosen_tests.loc[chosen_images, 'style'] = s\n\n        #assign leftover images to the last style in chosen_styles\n        if len(chosen_test_permutation) > 0:\n            chosen_tests.loc[chosen_test_permutation, 'style'] = chosen_styles[-1]\n\n        labels_wstyles = pd.concat((labels_wstyles, chosen_trains), ignore_index=True)\n        labels_wstyles = pd.concat((labels_wstyles, chosen_tests), ignore_index=True)\n\n    #for all entries, choose a style image of given style\n    style_location_list = []\n    for index,row in labels_wstyles.iterrows():\n        style_location_list.append(choose_style_image(location_styles, row['style'], row['split']))\n\n    style_location_df = pd.DataFrame(style_location_list, columns=['style_filename'])\n    labels_wstyles['style_filename'] = style_location_df\n\n    #TODO: add neural style transferoutput_location\n    #TODO: formalize output\n    output_label = os.path.join(output_location, 'label.csv')\n    output_data = os.path.join(output_location, \"data\")\n\n    #create folders necessary for output_data (and output_label)\n    os.makedirs(output_data, exist_ok=True)\n    \n    \n    labels_wstyles.to_csv(output_label)\n\n    #create stylised dataset\n    for index,row in labels_wstyles.iterrows():\n        location_style_image = os.path.join(location_styles, \"data\", row['style_filename'])\n        location_content_image = os.path.join(location, \"data\", row['filename'])\n        location_output_image = os.path.join(output_location, \"data\", row['filename'])\n        \n        with Image.open(location_content_image) as img:\n            width, height = img.size\n            #choosing the smaller value between image size, and the requested p_image_size\n            #target_image_size = int(max(width, height))\n            target_image_size = min(p_image_size, int(max(width, height)))\n            \n        \n        command = \"/kaggle/usr/lib/neural_style/neural_style.py -style_image %s -style_blend_weights %s -content_image %s -image_size %s -gpu %s -content_weight %s -style_weight %s normalize_weights %s -normalize_gradients %s -tv_weight %s -num_iterations %s -init %s -init_image %s -optimizer %s -learning_rate %s -lbfgs_num_correction %s -print_iter %s -save_iter %s -output_image %s -style_scale %s -original_colors %s -pooling %s -model_file %s -disable_check %s -backend %s -cudnn_autotune %s -seed %s -content_layers %s -style_layers %s -multidevice_strategy %s\" %(\n                location_style_image, p_style_blend_weights, location_content_image, target_image_size, p_gpu, p_content_weight, p_style_weight, p_normalize_weights, p_normalize_gradients, p_tv_weight, p_num_iterations, p_init, p_init_image, p_optimizer, p_learning_rate, p_lbfgs_num_correction, p_print_iter, p_save_iter, location_output_image, p_style_scale, p_original_colors, p_pooling, p_model_file, p_disable_check, p_backend, p_cudnn_autotune, p_seed, p_content_layers, p_style_layers, p_multidevice_strategy)\n        !python3 $command\n        print(\"Finished running: %s\" %command)\n                    \n    return 0","metadata":{"execution":{"iopub.status.busy":"2022-12-07T08:35:41.274134Z","iopub.execute_input":"2022-12-07T08:35:41.274641Z","iopub.status.idle":"2022-12-07T08:35:41.316287Z","shell.execute_reply.started":"2022-12-07T08:35:41.274607Z","shell.execute_reply":"2022-12-07T08:35:41.315298Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"p_model_location = \"/kaggle/working/models\"\ndownload_models.main(p_model_location)\np_model_file = os.path.join(p_model_location, \"vgg19-d01eb7cb.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-12-07T08:35:42.735065Z","iopub.execute_input":"2022-12-07T08:35:42.736083Z","iopub.status.idle":"2022-12-07T08:35:42.742157Z","shell.execute_reply.started":"2022-12-07T08:35:42.736033Z","shell.execute_reply":"2022-12-07T08:35:42.741165Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"All models have been successfully downloaded\n","output_type":"stream"}]},{"cell_type":"code","source":"location = \"/kaggle/input/ter-set-1/archive/data/Human_Actions\"\nlocation_styles = \"/kaggle/input/ter-set-1/Classified_Style_Dataset/Classified_Style_Dataset\"\nn_classes = 3\nn_styles = 3\nn_train = 40\nn_test = 39\np_train = 0.9\noutput_location = \"/kaggle/working/output/Human_Actions_Stylized_Experiment_09122022\"\n\ncreate_stylized_dataset(location, location_styles, n_classes, n_styles, n_train, n_test, p_train, output_location,\n                        p_model_file = p_model_file, p_original_colors = \"1\", p_style_weight=25, p_image_size=512\n        #the rest are remaining neural style transfer arguments\n                       )\n\"\"\"\n        ,p_style_weight=1e2, p_content_weight=5e0, p_num_iterations=1000, p_learning_rate = 1e0, \n        p_gpu=0, p_image_size=512, p_style_blend_weights=None, p_normalize_weights=False, p_normalize_gradients=False, p_tv_weight=1e-3, p_init='random', p_init_image=None, p_optimizer='lbfgs', \n        p_lbfgs_num_correction=100,\n        p_print_iter=0, p_save_iter=0, p_style_scale=1.0, p_original_colors = 0, p_model_file='models/vgg19-d01eb7cb.pth', p_disable_check=False, \n        p_backend='nn', p_cudnn_autotune=False, p_pooling='max',\n        p_seed=-1, p_content_layers='relu4_2', p_style_layers='relu1_1,relu2_1,relu3_1,relu4_1,relu5_1', p_multidevice_strategy='4,7,29')\n\"\"\"\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T08:35:45.745418Z","iopub.execute_input":"2022-12-07T08:35:45.745783Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"All models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Cathedral_5.png -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_092.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_092.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_4.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_165.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_165.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_Grey.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_059.jpg -image_size 444 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_059.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_Grey.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_041.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_041.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_4.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_166.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_166.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Circuit_Board_3.png -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_098.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_098.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_4.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_171.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_171.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_Grey.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_194.jpg -image_size 450 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_194.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_4.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_083.jpg -image_size 472 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_083.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_Grey.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_075.jpg -image_size 459 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_075.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_4.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_242.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_242.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_4.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_250.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_250.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_Grey.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_178.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_178.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_4.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_102.jpg -image_size 286 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_102.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Circuit_Board_2.jpeg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_155.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_155.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_Grey.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_079.jpg -image_size 500 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_079.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_Grey.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_045.jpg -image_size 451 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_045.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_Grey.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_103.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_103.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_4.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_232.jpg -image_size 376 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_232.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_Grey.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_070.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_070.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_Grey.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_245.jpg -image_size 512 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_245.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_Grey.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_162.jpg -image_size 451 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_162.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\nFinished running: /kaggle/usr/lib/neural_style/neural_style.py -style_image /kaggle/input/ter-set-1/Style_Dataset/Style_Dataset/data/Soil_4.jpg -style_blend_weights None -content_image /kaggle/input/ter-set-1/archive/data/Human_Actions/data/running_100.jpg -image_size 452 -gpu 0 -content_weight 5e0 -style_weight 25 normalize_weights False -normalize_gradients False -tv_weight 1e-3 -num_iterations 1000 -init random -init_image None -optimizer lbfgs -learning_rate 1e0 -lbfgs_num_correction 100 -print_iter 0 -save_iter 0 -output_image /kaggle/working/output/Human_Actions_Stylized_Experiment/data/running_100.jpg -style_scale 1.0 -original_colors 1 -pooling max -model_file /kaggle/working/models/vgg19-d01eb7cb.pth -disable_check False -backend nn -cudnn_autotune False -seed -1 -content_layers relu4_2 -style_layers relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 -multidevice_strategy 4,7,29\nAll models have been successfully downloaded\nVGG-19 Architecture Detected\nSuccessfully loaded /kaggle/working/models/vgg19-d01eb7cb.pth\nCapturing style target 1\nRunning optimization with L-BFGS\n","output_type":"stream"}]},{"cell_type":"code","source":"#shutil.rmtree(\"/kaggle/working/output/\")\n#os.remove(\"/kaggle/working/output_archive.zip\")\n#os.makedirs(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2022-12-07T11:08:51.261319Z","iopub.execute_input":"2022-12-07T11:08:51.261997Z","iopub.status.idle":"2022-12-07T11:08:51.266517Z","shell.execute_reply.started":"2022-12-07T11:08:51.261959Z","shell.execute_reply":"2022-12-07T11:08:51.265556Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/output_archive\", 'zip', \"/kaggle/working/output\")","metadata":{"execution":{"iopub.status.busy":"2022-12-07T11:07:01.066383Z","iopub.execute_input":"2022-12-07T11:07:01.067135Z","iopub.status.idle":"2022-12-07T11:07:01.168974Z","shell.execute_reply.started":"2022-12-07T11:07:01.067098Z","shell.execute_reply":"2022-12-07T11:07:01.167878Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/output_archive.zip'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-12-07T11:06:43.591638Z","iopub.execute_input":"2022-12-07T11:06:43.592001Z","iopub.status.idle":"2022-12-07T11:06:43.600873Z","shell.execute_reply.started":"2022-12-07T11:06:43.591963Z","shell.execute_reply":"2022-12-07T11:06:43.599685Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"52"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}